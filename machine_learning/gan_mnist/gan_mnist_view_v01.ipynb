{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonsoomoon/anaconda/envs/tf1.5/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gan_mnist import GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,097,744\n",
      "Trainable params: 1,095,184\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gonsoomoon/anaconda/envs/tf1.5/lib/python3.6/site-packages/keras/engine/training.py:973: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.650923, acc.: 46.88%] [G loss: 0.589696]\n",
      "1 [D loss: 0.464904, acc.: 68.75%] [G loss: 0.669890]\n",
      "2 [D loss: 0.359090, acc.: 73.44%] [G loss: 0.806400]\n",
      "3 [D loss: 0.299226, acc.: 85.94%] [G loss: 0.962708]\n",
      "4 [D loss: 0.249237, acc.: 89.06%] [G loss: 1.158433]\n",
      "5 [D loss: 0.218885, acc.: 95.31%] [G loss: 1.347030]\n",
      "6 [D loss: 0.191666, acc.: 98.44%] [G loss: 1.399816]\n",
      "7 [D loss: 0.144951, acc.: 100.00%] [G loss: 1.571798]\n",
      "8 [D loss: 0.149467, acc.: 98.44%] [G loss: 1.697047]\n",
      "9 [D loss: 0.116234, acc.: 100.00%] [G loss: 1.758151]\n",
      "10 [D loss: 0.096834, acc.: 100.00%] [G loss: 1.933143]\n",
      "11 [D loss: 0.089644, acc.: 100.00%] [G loss: 1.893710]\n",
      "12 [D loss: 0.091065, acc.: 100.00%] [G loss: 2.009366]\n",
      "13 [D loss: 0.097275, acc.: 100.00%] [G loss: 2.057205]\n",
      "14 [D loss: 0.074857, acc.: 100.00%] [G loss: 2.223622]\n",
      "15 [D loss: 0.063700, acc.: 100.00%] [G loss: 2.296654]\n",
      "16 [D loss: 0.066066, acc.: 100.00%] [G loss: 2.259883]\n",
      "17 [D loss: 0.049743, acc.: 100.00%] [G loss: 2.362595]\n",
      "18 [D loss: 0.057161, acc.: 100.00%] [G loss: 2.374877]\n",
      "19 [D loss: 0.049907, acc.: 100.00%] [G loss: 2.592538]\n",
      "20 [D loss: 0.056761, acc.: 100.00%] [G loss: 2.535959]\n",
      "21 [D loss: 0.053122, acc.: 100.00%] [G loss: 2.569995]\n",
      "22 [D loss: 0.045048, acc.: 100.00%] [G loss: 2.530873]\n",
      "23 [D loss: 0.045946, acc.: 100.00%] [G loss: 2.792044]\n",
      "24 [D loss: 0.040958, acc.: 100.00%] [G loss: 2.750711]\n",
      "25 [D loss: 0.043183, acc.: 100.00%] [G loss: 2.790131]\n",
      "26 [D loss: 0.041464, acc.: 100.00%] [G loss: 2.801686]\n",
      "27 [D loss: 0.039401, acc.: 100.00%] [G loss: 2.873651]\n",
      "28 [D loss: 0.032289, acc.: 100.00%] [G loss: 2.888361]\n",
      "29 [D loss: 0.029016, acc.: 100.00%] [G loss: 2.919527]\n",
      "30 [D loss: 0.032582, acc.: 100.00%] [G loss: 2.925853]\n",
      "31 [D loss: 0.031572, acc.: 100.00%] [G loss: 3.184129]\n",
      "32 [D loss: 0.028976, acc.: 100.00%] [G loss: 3.111472]\n",
      "33 [D loss: 0.025791, acc.: 100.00%] [G loss: 3.088168]\n",
      "34 [D loss: 0.031027, acc.: 100.00%] [G loss: 3.195623]\n",
      "35 [D loss: 0.023263, acc.: 100.00%] [G loss: 3.248389]\n",
      "36 [D loss: 0.035580, acc.: 100.00%] [G loss: 3.422077]\n",
      "37 [D loss: 0.018201, acc.: 100.00%] [G loss: 3.327362]\n",
      "38 [D loss: 0.026587, acc.: 100.00%] [G loss: 3.257653]\n",
      "39 [D loss: 0.030232, acc.: 100.00%] [G loss: 3.388658]\n",
      "40 [D loss: 0.026579, acc.: 100.00%] [G loss: 3.349271]\n",
      "41 [D loss: 0.021178, acc.: 100.00%] [G loss: 3.423794]\n",
      "42 [D loss: 0.023861, acc.: 100.00%] [G loss: 3.559846]\n",
      "43 [D loss: 0.026477, acc.: 100.00%] [G loss: 3.451594]\n",
      "44 [D loss: 0.021147, acc.: 100.00%] [G loss: 3.516172]\n",
      "45 [D loss: 0.016300, acc.: 100.00%] [G loss: 3.561475]\n",
      "46 [D loss: 0.015814, acc.: 100.00%] [G loss: 3.639619]\n",
      "47 [D loss: 0.023306, acc.: 100.00%] [G loss: 3.722749]\n",
      "48 [D loss: 0.019872, acc.: 100.00%] [G loss: 3.615225]\n",
      "49 [D loss: 0.015330, acc.: 100.00%] [G loss: 3.701165]\n",
      "50 [D loss: 0.020249, acc.: 100.00%] [G loss: 3.640455]\n",
      "51 [D loss: 0.021206, acc.: 100.00%] [G loss: 3.532527]\n",
      "52 [D loss: 0.023645, acc.: 100.00%] [G loss: 3.898037]\n",
      "53 [D loss: 0.016492, acc.: 100.00%] [G loss: 3.838813]\n",
      "54 [D loss: 0.018449, acc.: 100.00%] [G loss: 3.824201]\n",
      "55 [D loss: 0.015849, acc.: 100.00%] [G loss: 3.821036]\n",
      "56 [D loss: 0.016886, acc.: 100.00%] [G loss: 3.875460]\n",
      "57 [D loss: 0.014274, acc.: 100.00%] [G loss: 3.952820]\n",
      "58 [D loss: 0.017789, acc.: 100.00%] [G loss: 3.909738]\n",
      "59 [D loss: 0.012286, acc.: 100.00%] [G loss: 3.910602]\n",
      "60 [D loss: 0.013155, acc.: 100.00%] [G loss: 3.981530]\n",
      "61 [D loss: 0.012757, acc.: 100.00%] [G loss: 3.939834]\n",
      "62 [D loss: 0.019503, acc.: 100.00%] [G loss: 3.984314]\n",
      "63 [D loss: 0.017289, acc.: 100.00%] [G loss: 3.990108]\n",
      "64 [D loss: 0.015274, acc.: 100.00%] [G loss: 3.941226]\n",
      "65 [D loss: 0.015570, acc.: 100.00%] [G loss: 3.933936]\n",
      "66 [D loss: 0.011279, acc.: 100.00%] [G loss: 4.056706]\n",
      "67 [D loss: 0.015591, acc.: 100.00%] [G loss: 4.044005]\n",
      "68 [D loss: 0.014056, acc.: 100.00%] [G loss: 4.094825]\n",
      "69 [D loss: 0.011504, acc.: 100.00%] [G loss: 4.136404]\n",
      "70 [D loss: 0.017269, acc.: 100.00%] [G loss: 4.113466]\n",
      "71 [D loss: 0.011645, acc.: 100.00%] [G loss: 4.199309]\n",
      "72 [D loss: 0.013113, acc.: 100.00%] [G loss: 4.338766]\n",
      "73 [D loss: 0.010661, acc.: 100.00%] [G loss: 4.195205]\n",
      "74 [D loss: 0.013677, acc.: 100.00%] [G loss: 4.196609]\n",
      "75 [D loss: 0.011569, acc.: 100.00%] [G loss: 4.119209]\n",
      "76 [D loss: 0.016711, acc.: 100.00%] [G loss: 4.149682]\n",
      "77 [D loss: 0.012444, acc.: 100.00%] [G loss: 4.295103]\n",
      "78 [D loss: 0.012029, acc.: 100.00%] [G loss: 4.306429]\n",
      "79 [D loss: 0.014004, acc.: 100.00%] [G loss: 4.404804]\n",
      "80 [D loss: 0.009862, acc.: 100.00%] [G loss: 4.349426]\n",
      "81 [D loss: 0.010305, acc.: 100.00%] [G loss: 4.342341]\n",
      "82 [D loss: 0.015541, acc.: 100.00%] [G loss: 4.286858]\n",
      "83 [D loss: 0.011368, acc.: 100.00%] [G loss: 4.351071]\n",
      "84 [D loss: 0.011471, acc.: 100.00%] [G loss: 4.367743]\n",
      "85 [D loss: 0.007360, acc.: 100.00%] [G loss: 4.450358]\n",
      "86 [D loss: 0.009796, acc.: 100.00%] [G loss: 4.345306]\n",
      "87 [D loss: 0.011734, acc.: 100.00%] [G loss: 4.265307]\n",
      "88 [D loss: 0.016426, acc.: 100.00%] [G loss: 4.464352]\n",
      "89 [D loss: 0.013894, acc.: 100.00%] [G loss: 4.550604]\n",
      "90 [D loss: 0.010033, acc.: 100.00%] [G loss: 4.594091]\n",
      "91 [D loss: 0.013232, acc.: 100.00%] [G loss: 4.556864]\n",
      "92 [D loss: 0.013866, acc.: 100.00%] [G loss: 4.616313]\n",
      "93 [D loss: 0.010920, acc.: 100.00%] [G loss: 4.727727]\n",
      "94 [D loss: 0.012300, acc.: 100.00%] [G loss: 4.616949]\n",
      "95 [D loss: 0.008357, acc.: 100.00%] [G loss: 4.688738]\n",
      "96 [D loss: 0.009009, acc.: 100.00%] [G loss: 4.593875]\n",
      "97 [D loss: 0.012915, acc.: 100.00%] [G loss: 4.744474]\n",
      "98 [D loss: 0.007795, acc.: 100.00%] [G loss: 4.732660]\n",
      "99 [D loss: 0.014174, acc.: 100.00%] [G loss: 4.814119]\n",
      "100 [D loss: 0.008498, acc.: 100.00%] [G loss: 4.698184]\n",
      "101 [D loss: 0.009008, acc.: 100.00%] [G loss: 4.539210]\n",
      "102 [D loss: 0.012948, acc.: 100.00%] [G loss: 4.639990]\n",
      "103 [D loss: 0.020531, acc.: 100.00%] [G loss: 4.938105]\n",
      "104 [D loss: 0.010674, acc.: 100.00%] [G loss: 4.972862]\n",
      "105 [D loss: 0.010281, acc.: 100.00%] [G loss: 4.881887]\n",
      "106 [D loss: 0.009307, acc.: 100.00%] [G loss: 4.869327]\n",
      "107 [D loss: 0.008943, acc.: 100.00%] [G loss: 4.895528]\n",
      "108 [D loss: 0.008118, acc.: 100.00%] [G loss: 4.889496]\n",
      "109 [D loss: 0.017200, acc.: 100.00%] [G loss: 4.975109]\n",
      "110 [D loss: 0.010732, acc.: 100.00%] [G loss: 5.119133]\n",
      "111 [D loss: 0.018398, acc.: 100.00%] [G loss: 5.044606]\n",
      "112 [D loss: 0.011653, acc.: 100.00%] [G loss: 4.657103]\n",
      "113 [D loss: 0.015720, acc.: 100.00%] [G loss: 4.986008]\n",
      "114 [D loss: 0.007046, acc.: 100.00%] [G loss: 5.062711]\n",
      "115 [D loss: 0.016784, acc.: 100.00%] [G loss: 4.857063]\n",
      "116 [D loss: 0.016485, acc.: 100.00%] [G loss: 4.674682]\n",
      "117 [D loss: 0.011976, acc.: 100.00%] [G loss: 4.736425]\n",
      "118 [D loss: 0.012471, acc.: 100.00%] [G loss: 4.883397]\n",
      "119 [D loss: 0.010941, acc.: 100.00%] [G loss: 4.970963]\n",
      "120 [D loss: 0.021250, acc.: 100.00%] [G loss: 4.805853]\n",
      "121 [D loss: 0.022219, acc.: 100.00%] [G loss: 4.970233]\n",
      "122 [D loss: 0.030063, acc.: 100.00%] [G loss: 5.124765]\n",
      "123 [D loss: 0.044436, acc.: 100.00%] [G loss: 5.162852]\n",
      "124 [D loss: 0.056137, acc.: 100.00%] [G loss: 4.321766]\n",
      "125 [D loss: 0.017109, acc.: 100.00%] [G loss: 5.025204]\n",
      "126 [D loss: 0.010485, acc.: 100.00%] [G loss: 5.196246]\n",
      "127 [D loss: 0.044250, acc.: 98.44%] [G loss: 5.211740]\n",
      "128 [D loss: 0.063665, acc.: 98.44%] [G loss: 4.598392]\n",
      "129 [D loss: 0.018801, acc.: 100.00%] [G loss: 5.303751]\n",
      "130 [D loss: 0.127313, acc.: 95.31%] [G loss: 4.869348]\n",
      "131 [D loss: 0.022759, acc.: 100.00%] [G loss: 5.112887]\n",
      "132 [D loss: 0.301992, acc.: 90.62%] [G loss: 4.150238]\n",
      "133 [D loss: 0.027391, acc.: 98.44%] [G loss: 5.549714]\n",
      "134 [D loss: 0.095245, acc.: 98.44%] [G loss: 4.420969]\n",
      "135 [D loss: 0.087853, acc.: 93.75%] [G loss: 5.407500]\n",
      "136 [D loss: 0.752156, acc.: 76.56%] [G loss: 4.044439]\n",
      "137 [D loss: 0.026034, acc.: 100.00%] [G loss: 5.539973]\n",
      "138 [D loss: 1.662616, acc.: 37.50%] [G loss: 2.170945]\n",
      "139 [D loss: 1.398297, acc.: 64.06%] [G loss: 1.692989]\n",
      "140 [D loss: 0.444920, acc.: 79.69%] [G loss: 2.656655]\n",
      "141 [D loss: 0.122286, acc.: 96.88%] [G loss: 3.301663]\n",
      "142 [D loss: 0.079950, acc.: 98.44%] [G loss: 3.545988]\n",
      "143 [D loss: 0.135687, acc.: 92.19%] [G loss: 3.271306]\n",
      "144 [D loss: 0.087082, acc.: 96.88%] [G loss: 3.391120]\n",
      "145 [D loss: 0.062232, acc.: 100.00%] [G loss: 3.574063]\n",
      "146 [D loss: 0.088758, acc.: 96.88%] [G loss: 3.488790]\n",
      "147 [D loss: 0.174597, acc.: 95.31%] [G loss: 3.057667]\n",
      "148 [D loss: 0.117565, acc.: 96.88%] [G loss: 3.199138]\n",
      "149 [D loss: 0.109203, acc.: 100.00%] [G loss: 3.339172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 [D loss: 0.202501, acc.: 96.88%] [G loss: 2.886526]\n",
      "151 [D loss: 0.126626, acc.: 93.75%] [G loss: 3.263623]\n",
      "152 [D loss: 0.155808, acc.: 95.31%] [G loss: 3.007380]\n",
      "153 [D loss: 0.088977, acc.: 100.00%] [G loss: 3.648651]\n",
      "154 [D loss: 0.266807, acc.: 87.50%] [G loss: 2.688837]\n",
      "155 [D loss: 0.121029, acc.: 96.88%] [G loss: 3.255950]\n",
      "156 [D loss: 0.307559, acc.: 84.38%] [G loss: 3.321535]\n",
      "157 [D loss: 0.321684, acc.: 85.94%] [G loss: 2.799890]\n",
      "158 [D loss: 0.105115, acc.: 96.88%] [G loss: 3.629931]\n",
      "159 [D loss: 0.232604, acc.: 93.75%] [G loss: 2.972617]\n",
      "160 [D loss: 0.121091, acc.: 96.88%] [G loss: 3.381294]\n",
      "161 [D loss: 0.185594, acc.: 95.31%] [G loss: 3.449782]\n",
      "162 [D loss: 0.188134, acc.: 93.75%] [G loss: 3.139202]\n",
      "163 [D loss: 0.106243, acc.: 96.88%] [G loss: 3.648449]\n",
      "164 [D loss: 0.119454, acc.: 100.00%] [G loss: 3.606436]\n",
      "165 [D loss: 0.134978, acc.: 98.44%] [G loss: 3.326002]\n",
      "166 [D loss: 0.127388, acc.: 92.19%] [G loss: 4.216063]\n",
      "167 [D loss: 0.640383, acc.: 71.88%] [G loss: 1.904213]\n",
      "168 [D loss: 0.294595, acc.: 87.50%] [G loss: 3.159812]\n",
      "169 [D loss: 0.131439, acc.: 95.31%] [G loss: 5.240792]\n",
      "170 [D loss: 0.708706, acc.: 71.88%] [G loss: 1.496118]\n",
      "171 [D loss: 0.473084, acc.: 76.56%] [G loss: 2.150180]\n",
      "172 [D loss: 0.122809, acc.: 93.75%] [G loss: 4.005460]\n",
      "173 [D loss: 0.082550, acc.: 100.00%] [G loss: 4.289883]\n",
      "174 [D loss: 0.152340, acc.: 93.75%] [G loss: 4.042552]\n",
      "175 [D loss: 0.150143, acc.: 92.19%] [G loss: 4.151419]\n",
      "176 [D loss: 0.150006, acc.: 96.88%] [G loss: 3.571370]\n",
      "177 [D loss: 0.104916, acc.: 95.31%] [G loss: 3.716614]\n",
      "178 [D loss: 0.247678, acc.: 90.62%] [G loss: 3.764813]\n",
      "179 [D loss: 0.370484, acc.: 78.12%] [G loss: 3.057559]\n",
      "180 [D loss: 0.079852, acc.: 100.00%] [G loss: 4.004475]\n",
      "181 [D loss: 0.235197, acc.: 90.62%] [G loss: 2.558874]\n",
      "182 [D loss: 0.134937, acc.: 93.75%] [G loss: 3.400165]\n",
      "183 [D loss: 0.286902, acc.: 87.50%] [G loss: 3.182113]\n",
      "184 [D loss: 0.236759, acc.: 95.31%] [G loss: 3.396441]\n",
      "185 [D loss: 0.317004, acc.: 89.06%] [G loss: 3.325813]\n",
      "186 [D loss: 0.105077, acc.: 96.88%] [G loss: 3.310289]\n",
      "187 [D loss: 0.328822, acc.: 82.81%] [G loss: 4.219570]\n",
      "188 [D loss: 0.753994, acc.: 59.38%] [G loss: 1.197848]\n",
      "189 [D loss: 0.592174, acc.: 67.19%] [G loss: 3.286113]\n",
      "190 [D loss: 0.049797, acc.: 100.00%] [G loss: 4.741750]\n",
      "191 [D loss: 0.787635, acc.: 57.81%] [G loss: 1.289779]\n",
      "192 [D loss: 0.401200, acc.: 76.56%] [G loss: 2.416409]\n",
      "193 [D loss: 0.063109, acc.: 100.00%] [G loss: 4.324166]\n",
      "194 [D loss: 0.303649, acc.: 90.62%] [G loss: 2.404592]\n",
      "195 [D loss: 0.149725, acc.: 96.88%] [G loss: 3.092332]\n",
      "196 [D loss: 0.105734, acc.: 98.44%] [G loss: 3.989266]\n",
      "197 [D loss: 0.215787, acc.: 90.62%] [G loss: 3.702471]\n",
      "198 [D loss: 0.177053, acc.: 93.75%] [G loss: 3.495058]\n",
      "199 [D loss: 0.101963, acc.: 98.44%] [G loss: 3.793248]\n",
      "200 [D loss: 0.212903, acc.: 93.75%] [G loss: 3.298867]\n",
      "201 [D loss: 0.231143, acc.: 92.19%] [G loss: 3.076916]\n",
      "202 [D loss: 0.446426, acc.: 78.12%] [G loss: 2.249159]\n",
      "203 [D loss: 0.146869, acc.: 96.88%] [G loss: 3.736261]\n",
      "204 [D loss: 0.489737, acc.: 75.00%] [G loss: 2.901804]\n",
      "205 [D loss: 0.148605, acc.: 92.19%] [G loss: 3.757492]\n",
      "206 [D loss: 0.277755, acc.: 90.62%] [G loss: 2.326158]\n",
      "207 [D loss: 0.196074, acc.: 95.31%] [G loss: 3.838972]\n",
      "208 [D loss: 0.281592, acc.: 89.06%] [G loss: 3.671989]\n",
      "209 [D loss: 0.273068, acc.: 85.94%] [G loss: 2.953080]\n",
      "210 [D loss: 0.181501, acc.: 95.31%] [G loss: 4.218200]\n",
      "211 [D loss: 0.799762, acc.: 56.25%] [G loss: 1.587113]\n",
      "212 [D loss: 0.206549, acc.: 90.62%] [G loss: 3.814643]\n",
      "213 [D loss: 0.346785, acc.: 85.94%] [G loss: 3.047341]\n",
      "214 [D loss: 0.141876, acc.: 95.31%] [G loss: 3.694083]\n",
      "215 [D loss: 0.769507, acc.: 60.94%] [G loss: 2.569101]\n",
      "216 [D loss: 0.170082, acc.: 96.88%] [G loss: 4.068444]\n",
      "217 [D loss: 0.973336, acc.: 46.88%] [G loss: 1.658435]\n",
      "218 [D loss: 0.244051, acc.: 87.50%] [G loss: 4.184886]\n",
      "219 [D loss: 0.543364, acc.: 73.44%] [G loss: 1.816316]\n",
      "220 [D loss: 0.225088, acc.: 92.19%] [G loss: 3.055234]\n",
      "221 [D loss: 0.398765, acc.: 82.81%] [G loss: 3.503741]\n",
      "222 [D loss: 0.510417, acc.: 71.88%] [G loss: 1.927957]\n",
      "223 [D loss: 0.200984, acc.: 93.75%] [G loss: 3.995015]\n",
      "224 [D loss: 1.110167, acc.: 39.06%] [G loss: 1.065195]\n",
      "225 [D loss: 0.315931, acc.: 81.25%] [G loss: 2.966454]\n",
      "226 [D loss: 0.283572, acc.: 93.75%] [G loss: 2.684141]\n",
      "227 [D loss: 0.303417, acc.: 90.62%] [G loss: 2.651048]\n",
      "228 [D loss: 0.266615, acc.: 87.50%] [G loss: 3.486755]\n",
      "229 [D loss: 0.685905, acc.: 60.94%] [G loss: 1.873639]\n",
      "230 [D loss: 0.174528, acc.: 98.44%] [G loss: 3.339212]\n",
      "231 [D loss: 0.645688, acc.: 65.62%] [G loss: 1.670343]\n",
      "232 [D loss: 0.239445, acc.: 87.50%] [G loss: 3.033765]\n",
      "233 [D loss: 0.541542, acc.: 71.88%] [G loss: 1.813823]\n",
      "234 [D loss: 0.334529, acc.: 81.25%] [G loss: 3.149424]\n",
      "235 [D loss: 0.615533, acc.: 60.94%] [G loss: 1.532635]\n",
      "236 [D loss: 0.384273, acc.: 78.12%] [G loss: 2.465637]\n",
      "237 [D loss: 0.921405, acc.: 51.56%] [G loss: 1.550026]\n",
      "238 [D loss: 0.338105, acc.: 85.94%] [G loss: 2.967637]\n",
      "239 [D loss: 1.106907, acc.: 31.25%] [G loss: 0.685141]\n",
      "240 [D loss: 0.447331, acc.: 73.44%] [G loss: 2.404964]\n",
      "241 [D loss: 0.661269, acc.: 59.38%] [G loss: 2.018370]\n",
      "242 [D loss: 0.554066, acc.: 70.31%] [G loss: 2.679519]\n",
      "243 [D loss: 0.883520, acc.: 40.62%] [G loss: 1.278816]\n",
      "244 [D loss: 0.466137, acc.: 70.31%] [G loss: 2.135767]\n",
      "245 [D loss: 0.655172, acc.: 59.38%] [G loss: 1.673491]\n",
      "246 [D loss: 0.588115, acc.: 62.50%] [G loss: 1.879069]\n",
      "247 [D loss: 0.675212, acc.: 62.50%] [G loss: 1.358943]\n",
      "248 [D loss: 0.508362, acc.: 71.88%] [G loss: 1.900544]\n",
      "249 [D loss: 0.696922, acc.: 56.25%] [G loss: 1.399253]\n",
      "250 [D loss: 0.470832, acc.: 79.69%] [G loss: 2.112173]\n",
      "251 [D loss: 0.886926, acc.: 43.75%] [G loss: 1.201340]\n",
      "252 [D loss: 0.467277, acc.: 71.88%] [G loss: 1.852117]\n",
      "253 [D loss: 0.773816, acc.: 54.69%] [G loss: 1.078977]\n",
      "254 [D loss: 0.783450, acc.: 50.00%] [G loss: 1.664966]\n",
      "255 [D loss: 0.696313, acc.: 62.50%] [G loss: 1.393124]\n",
      "256 [D loss: 0.761280, acc.: 50.00%] [G loss: 1.104685]\n",
      "257 [D loss: 0.630571, acc.: 59.38%] [G loss: 1.803408]\n",
      "258 [D loss: 0.743048, acc.: 56.25%] [G loss: 1.197591]\n",
      "259 [D loss: 0.566773, acc.: 62.50%] [G loss: 1.556863]\n",
      "260 [D loss: 0.888842, acc.: 42.19%] [G loss: 0.903854]\n",
      "261 [D loss: 0.551467, acc.: 65.62%] [G loss: 1.538770]\n",
      "262 [D loss: 0.880827, acc.: 37.50%] [G loss: 0.917957]\n",
      "263 [D loss: 0.590700, acc.: 57.81%] [G loss: 1.594615]\n",
      "264 [D loss: 0.831647, acc.: 50.00%] [G loss: 0.865850]\n",
      "265 [D loss: 0.516060, acc.: 71.88%] [G loss: 1.432066]\n",
      "266 [D loss: 0.627467, acc.: 57.81%] [G loss: 1.439240]\n",
      "267 [D loss: 0.806662, acc.: 48.44%] [G loss: 1.131621]\n",
      "268 [D loss: 0.543315, acc.: 71.88%] [G loss: 1.685053]\n",
      "269 [D loss: 0.976362, acc.: 34.38%] [G loss: 0.668696]\n",
      "270 [D loss: 0.665841, acc.: 54.69%] [G loss: 1.225453]\n",
      "271 [D loss: 0.685243, acc.: 50.00%] [G loss: 1.357247]\n",
      "272 [D loss: 0.774146, acc.: 42.19%] [G loss: 1.011423]\n",
      "273 [D loss: 0.607321, acc.: 64.06%] [G loss: 1.318663]\n",
      "274 [D loss: 0.718099, acc.: 53.12%] [G loss: 1.077970]\n",
      "275 [D loss: 0.685564, acc.: 54.69%] [G loss: 1.108825]\n",
      "276 [D loss: 0.782003, acc.: 45.31%] [G loss: 1.387679]\n",
      "277 [D loss: 0.876278, acc.: 32.81%] [G loss: 0.883439]\n",
      "278 [D loss: 0.656784, acc.: 57.81%] [G loss: 1.108587]\n",
      "279 [D loss: 0.722516, acc.: 57.81%] [G loss: 1.093425]\n",
      "280 [D loss: 0.714719, acc.: 51.56%] [G loss: 1.198438]\n",
      "281 [D loss: 0.817055, acc.: 40.62%] [G loss: 0.838471]\n",
      "282 [D loss: 0.681036, acc.: 50.00%] [G loss: 0.940410]\n",
      "283 [D loss: 0.684625, acc.: 50.00%] [G loss: 1.182189]\n",
      "284 [D loss: 0.722151, acc.: 51.56%] [G loss: 0.954322]\n",
      "285 [D loss: 0.718085, acc.: 46.88%] [G loss: 0.932697]\n",
      "286 [D loss: 0.637321, acc.: 56.25%] [G loss: 1.168507]\n",
      "287 [D loss: 0.748343, acc.: 45.31%] [G loss: 0.965266]\n",
      "288 [D loss: 0.675527, acc.: 56.25%] [G loss: 1.089392]\n",
      "289 [D loss: 0.761190, acc.: 45.31%] [G loss: 0.949216]\n",
      "290 [D loss: 0.625138, acc.: 57.81%] [G loss: 1.031616]\n",
      "291 [D loss: 0.670311, acc.: 50.00%] [G loss: 1.041089]\n",
      "292 [D loss: 0.754394, acc.: 42.19%] [G loss: 0.819503]\n",
      "293 [D loss: 0.697211, acc.: 48.44%] [G loss: 0.887068]\n",
      "294 [D loss: 0.637153, acc.: 53.12%] [G loss: 0.923190]\n",
      "295 [D loss: 0.597935, acc.: 57.81%] [G loss: 0.991389]\n",
      "296 [D loss: 0.661048, acc.: 56.25%] [G loss: 1.006759]\n",
      "297 [D loss: 0.621357, acc.: 64.06%] [G loss: 1.070453]\n",
      "298 [D loss: 0.807377, acc.: 35.94%] [G loss: 0.744741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299 [D loss: 0.613528, acc.: 54.69%] [G loss: 0.968755]\n",
      "Elapsed Time: 0.38 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "gan = GAN()\n",
    "start = time.time()\n",
    "gan.train(epochs=300, batch_size=32, sample_interval=200)\n",
    "end = time.time()\n",
    "print(\"Elapsed Time: %.2f minutes\" % ((end-start)/60))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
