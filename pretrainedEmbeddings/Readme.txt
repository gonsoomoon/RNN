	
1.Project 
 - A text classification using the pre-trained embeddngs in the bi-directional GRU

2.Description
 - As an initial value, using the pre-trained embeddings is more effective and efficient than random embeddings' values for most text classification problems. This paper compares the two situations using the bi-directional GRU (Gated Recurrent Units) and proves empirically the superiority of the pre-trained embeddings.

3.Date & Author
 - Oct 2017, Gonsoo Moon

4.Environment
 1) Mac Pro 2.6 GHz Intel Core i5, 16 GB RAM
 2) Tensorflow 1.0.0
 3) Python 3.6

5.How to run
 python GRU_pretrained_GloVe_moon.py 


6.Result
 - Refer to the project report

7.Reference
 [1] Nikhil Buduma (2017). Fundamentals of deep learning. Sebastopol, CA: Oâ€™Reilly Media
 [2] Jeffrey Pennington, Richard Socher, Christopher D. Manning (2015). GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/

	   


	
